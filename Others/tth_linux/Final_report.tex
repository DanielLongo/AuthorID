%File: AAAI-inst.tex
% WARNING: If you are not an experienced LaTeX user, AAAI does
% NOT recommend that you use LaTeX to format your paper. No
% support for LaTeX is provided by AAAI, and these instructions
% and the accompanying style files are NOT guaranteed to work.
% If the results you obtain are not in accordance with the 
% specifications you received in your packet (or online), you
% must correct the style files or macro to achieve the correct
% result. 
%
% AAAI CANNOT HELP YOU WITH THIS TASK. 
%
% The instructions herein are provided as a general guide for 
% experienced LaTeX users who would like to use that software
% to format their paper for an AAAI Press proceedings or technical
% report or AAAI working notes. These instructions are generic. 
% Consequently, they do not include specific dates, page charges, and so forth. 
% Please consult your specific written conference instructions for 
% details regarding your submission.
%
% Acknowledgments
% The preparation of the \LaTeX{} and Bib\TeX{} files that
% implement these instructions was supported by 
% Schlumberger Palo Alto Research, AT\&T Bell
% Laboratories, Morgan Kaufmann Publishers, and AAAI Press.
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{hyperref}
\usepackage{array}
\usepackage[square, sort, numbers]{natbib}
\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}
\begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Authorship Identification for Documents}
\author{
Devansh Dalal(2012CS10224)  and  Abhishek(2012CS10208)\\ 
\\
}


\maketitle
\begin{abstract}
\begin{quote}
Authorship identification has been a very important and practical problem in Natural Language Processing. The problem is to identify the author of a document from a given list of possible authors. A large amount of work exists on this problem in literature. We develop ideas based on this work in order to build our own model for authorship identification. We also take a model from this work as a baseline for comparing the results. Our model for the task is a text classifier based on logistic regression which includes n-grams, style markers and document finger-printing as features. \textit{Reuter\_50\_50} is the dataset used. The baseline achieves an accuracy of 73.08\% on this dataset. We were able to achieve a close accuracy of 72.11\%.
\end{quote}
\end{abstract}



\section{Introduction}
Authorship identification has been a very important and practical problem in Natural Language Processing. It has become even more important with the widespread use of various forums where anonymity is allowed. A good system for authorship identification can handle cases of misuse of anonymity. Such a system can also be used for plagiarism detection. Analysis of authorship of historical texts has already been done many times using techniques for authorship attribution. Various spoofing e-mails can also be detected by using these techniques.

\section{Previous Work}
The problem of authorship identification has been studied extensively. Many hand-designed features have been developed in order to tackle this problem. For solving this problem, conventional text classifiers like naive bayes, logistic regression and SVM have been used, and various deep learning models like LSTM have also been tried. Different models perform differently depending upon the type of training and testing data available.

We take the results of \cite{baseline} as baseline for comparison. We use the same data so that comparison makes sense. The work on authorship identification before \cite{baseline} used fixed length n-grams only. This work was different in the way that it used variable length n-grams as features. It mainly used 3-grams, 4-grams and 5-grams as it was well-established that subsequent n-grams don't impact the model much. After introducing the various n-gram features, it extracts the most frequently occuring n-grams using a feature selection method. The final feature values are fed into a linear SVM for classification. The accuracy obtained by this system for \textit{Reuter\_50\_50} dataset was 73.08\%. We have tried to replicate these results. However, the most recent work on this dataset is \cite{latest} where the authors have obtained an accuracy of upto 75.48\%. This model uses very complicated mathematical techniques which are completely different from the ones we have discussed in the class. Therefore, we did not choose it as our baseline.


\section{Dataset}
We have used the \textit{Reuter\_50\_50} dataset which was easily available at the website \cite{reuter}. It contains 50 training documents and 50 test documents for each of the 50 authors. All of these documents are on the same topic of news. Every document has been written by one and only one author.

\section{Our Model}
It is a supervised learning task as a label is present for each of the training examples. Since there are multiple authors and we have to assign the document just one of them, it is also a multi-class single label text categorization problem.

Our model for solving this problem is basically made up of these 3 components: logistic regression classifier, stylometric features or style markers, document finger-printing. Similar ideas have been tried out in the past but not necessarily in this particular combination. We have used Python programming language for implementing our model. We have used logistic regression classifier from the Python SciKit Learn library.

\subsection{Stylometric Features}
Stylometric features or style markers try to capture the writing style of the author. We have used a large number of stylometric features in our model. The motivation behind these features comes from the survey paper on authorship attribution \cite{survey}. The features pertaining to types and token are: number of types (unique words), number of tokens, type-token ratio. Some authors have a tendency to write long sentences whereas some authors prefer shorter sentences. In order to capture this, we have included various features in our model such as average length of a sentence (both in terms of words and characters), standard deviation of sentence lengths. Some authors pose more questions as compared to others. For assimilating such ideas in our model we have introduced features like relative number of declarative, interrogative, imperative and exclamatory sentences. Other style markers include relative number of punctuations, relative number of digits in the document etc. Natural Language Tool Kit (NLTK) has been used to extract these style markers from the text. 

\subsection{Document Finger-printing}
Fingerprint identification is a well-known technique in forensic sciences. The basic idea of identifying a subject based on a set of features left by the subject actions or behavior can also be applied to other domains. Identifying text authorship based on an author “fingerprint” is one such application. Finger-printing has mainly been used in the past for plagiarism detection. Basically, the idea is to generate a compact signature for each document and then those signatures can be matched for similarity.

We used 2 types of document finger-printing, character level finger-print generation and word n-gram level finger-printing. In character level finger-printing, we first create a set of all $k (k>=5)$ length character n-grams. Then we select a small subset of these character n-grams based on their hash values using the Winnow's algorithm\cite{schleimer2003winnowing} implemented in this python fingerprint library\cite{finger}. The hash value of the sequence of chosen character n-grams is returned as the signature of the document. To make the process efficient we used \href{https://github.com/kailashbuki/fingerprint}{fingerprint} library. Similarly word level features can be used and similarity between 2 documents can be computed as the number of matches found. This word-level analysis is very slow if implemented normally. So, we used DARMC Intertextuality Analysis Library\cite{DARMC} for effective computation of the similarity function.


\subsection{EXPERIMENTATION AND RESULTS}
The implementation was done in Python using the commonly used ML and NLP libraries like scikit learn, Theano , NLTK etc. The code is available in \href{https://github.com/devanshdalal/Author-Identification-task}{this github repository}.

% Please write how we added those incremental things , what were the motivation to add, results were positive (explain), you may give accuracies values here but donot draw table.[it is in conclusion].

We started out with the simple naive bayes classifier which gave a decent accuracy. Then we tried various other classifier models of which logistic regression performed the best for us. So we decided to go ahead with it. Then we added various stylometric features to our model. This led to a considerable increase in accuracy. After that we went further to add the document finger-printing features which again led to a significant increase in accuracy.

The accuracies obtained on the various models mentioned above have been tabulated in Table 1.


\begin{table}[h!]
\centering
\begin{tabular}{||c | c||} 
 \hline
 model & accuracy \\ [0.5ex] 
 \hline\hline
 NB & 67.15\%   \\
 \hline
 SVM & 69.1\%   \\
 \hline
 Logistic Regression (LR)  & 70.76 \% \\
  \hline
 LR + stylometric features & 71.36 \% \\
 \hline
 \multicolumn{1}{||m{4cm}|}{LR + stylometric features + document finger-printing} & 72.11 \% \\
 \hline
\end{tabular}
\caption{ accuracies of models with progressive features }
\label{table:1}
\end{table}



\subsection{OTHER METHODS EXPLORED}
\subsubsection{Long short-term memory (LSTM)} 
We also tried LSTMs as they have been proved to be very effective for this problem in the literature. Implementation of LSTM was done in theano, by modifying the tutorial code given for LSTM on theano official website \cite{hochreiter1997long}. The initial accuracies were not good. One problem that we suffered from was the lack of data. We had only 16 MBs of text, used for training and testing of 50 classes. Since LSTMs require a large amount of data for getting appreciable results, they did not help in effectively solving our problem. Therefore, we did not further consider the approach of using LSTMs for solving our problem. Table 2 lists some results for LSTM.

\begin{table}[h!]
\centering
\begin{tabular}{||c | c||} 
 \hline
 classes & accuracy \\ [0.5ex] 
 \hline\hline
 first vs all & 92.5\%   \\
 \hline
 first vs second & 85\%   \\
 \hline
 5 authors  & 67.2 \% \\
 \hline
\end{tabular}
\caption{ results for basic LSTM architecture }
\label{table:1}
\end{table}



\subsubsection{Knowledge of most confused authors}
Due to data insufficiency, we restricted ourselves to simpler models that use more number of features. For performing error analysis on our model, we constructed confusion matrix for the authors. From there we tried to use the knowledge of most confused authors for improving the performance of our model. One key observation was that the confusion matrix was a sparse matrix. This means that an author is confused with only a small number of other authors and not with almost every other author. In particular, there are certain pairs of authors who are confused with each other. For disambiguating between such authors we trained smaller models made exclusively for those authors. First a confusion matrix is created for the predictions on the training set. Then from the confusion matrix we pick top-k most confused pairs of authors (depending on the total number of mis-predictions among them). We train smaller models on each of these selected pairs. Now for prediction on a test example, if the predicted author is found to be one of the confused authors, we use the majority of the outputs from the smaller models. This method increased accuracy by a small amount. However, this method is computationally expensive and therefore we could not use it effectively.

\subsubsection{Lemmatization and POS tagging}
We considered replacing the words with their root forms using Porter Stemmer lemmatization \cite{stem}. But it did not result in an increase in accuracy. We also tried the syntactic feature of appending the words with their Part-Of-Speech (POS) tags. Again this idea did not work out in our favour. These features try to hide the actual word information. Perhaps due to this they did not improve the accuracy.

\section{CONCLUSION}
The accuracy obtained by our model is close to the accuracy of the baseline model. This shows that stylometric features and finger-printing features are crucial to the task of authorship identification. Also for the used dataset, traditional classifiers like naive Bayes, SVM and logistic regression are performing better than neural network models like LSTM.

\section{FUTURE WORK}
One promising approach is to use LSTMs with more amount of data and  to build on the previous work\cite{pandeycs671}\cite{jindaldeanonymizing} added with all the stylometric features and document finger-printing ideas. Also the idea of using the confusion knowledge from training data for disambiguating between certain pairs of confusing authors seems to improve the results. The document similarity function is still very slow and therefore we are using only a part of the text written by an author as the representative text for that author. With more efficient methods , it might be possible to include more text for each author. Since we need a lot of data for more complex models, work can be focused on creating a large authorship dataset, preferably using a web crawler. One potential source of authorship data is Quora.

The classifiers that we used were linear in nature. Therefore, it seems that the use of non-linear models can increase accuracy as non-linear models will also be able to capture non-linear properties of the documents. Adding semantic features like synonyms and antonyms can also be tried.



\bigskip


% \newpage
\medskip


\bibliography{bibfile}
% \bibliographystyle{aaai}
% \bibliographystyle{ieeer}
\bibliographystyle{unsrt}

\end{document}
